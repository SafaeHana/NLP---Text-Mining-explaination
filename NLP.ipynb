{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3564291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading NLTK Corpora\n",
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd6c1964",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It', 'is', 'not', 'news', 'that', 'Nathan', ...]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_words=brown.words(categories='reviews')\n",
    "reviews_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2e9f3823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40704"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reveiws_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d99dbe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello word!', 'this is a sentence tokenizing']\n"
     ]
    }
   ],
   "source": [
    "#Text extraction and preprocessing : Sentence Tokenization \n",
    "from nltk.tokenize import sent_tokenize\n",
    "Example_sentence=\"Hello word! this is a sentence tokenizing\"\n",
    "print(sent_tokenize(Example_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7c3ad3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'word', '!', 'this', 'is', 'a', 'word', 'tokenizing']\n"
     ]
    }
   ],
   "source": [
    "#Text extraction and preprocessing : word Tokenization \n",
    "from nltk.tokenize import word_tokenize\n",
    "Example_word=\"Hello word! this is a word tokenizing\"\n",
    "print(word_tokenize(Example_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "eaf6d73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text extraction and preprocessing : Stop Word Removal\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a2dd9072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'day', 'keeps', 'diseases', 'bay', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "example_sent=\"an apple a day keeps diseases at bay.\"\n",
    "\n",
    "stop_words=set(stopwords.words('english'))\n",
    "\n",
    "word_tokens= word_tokenize(example_sent)\n",
    "#filtered_sent=[w for w in word_tokens if not w in stop_words]  this ligne is equivant to the code below :)\n",
    "filtered_sent=[]\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)\n",
    "       \n",
    "print(filtered_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3e5086bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "important\n",
      "of\n",
      "cave\n",
      "as\n",
      "explain\n",
      "by\n",
      "caver\n"
     ]
    }
   ],
   "source": [
    "#Text extraction and preprocessing : Stemming\n",
    "#Various stemming algorithms: Porter stemmer, lancaster stemmer, Snowball stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "ps=PorterStemmer()\n",
    "new_text=\"Importantance of caving as explained by cavers\"\n",
    "\n",
    "words=word_tokenize(new_text)\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "21c3bbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foot\n",
      "loving\n",
      "love\n"
     ]
    }
   ],
   "source": [
    "#Text extraction and preprocessing : Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"feet\"))\n",
    "print(lemmatizer.lemmatize(\"loving\"))\n",
    "print(lemmatizer.lemmatize(\"loving\",\"v\")) #lemmatize with POS tag , v:verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b59e7f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('process', 'NN'), ('labeling', 'VBG'), ('word', 'NN'), ('text', 'NN'), ('document', 'NN'), ('corresponding', 'VBG'), ('part', 'NN'), ('speech', 'NN'), (',', ','), ('noun', 'RB'), (',', ','), ('verb', 'NN'), (',', ','), ('adjective', 'JJ'), (',', ','), ('adverb', 'NN'), ('.', '.')]\n",
      "[('POS', 'NNP'), ('tagging', 'VBG'), ('important', 'JJ'), ('many', 'JJ'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('tasks', 'NNS'), (',', ','), ('text', 'JJ'), ('classification', 'NN'), ('information', 'NN'), ('retrieval', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#Text extraction and preprocessing : POS Tagging:Part-of-speech\n",
    "from nltk import pos_tag\n",
    "stop_words=set(stopwords.words('english'))\n",
    "\n",
    "txt='''The process of labeling each word in a text document with \n",
    "its corresponding part of speech, such as noun, verb, adjective, or adverb. \n",
    "POS tagging is important for many natural language processing tasks, \n",
    "such as text classification and information retrieval.\n",
    "'''\n",
    "tokenized=sent_tokenize(txt)\n",
    "for i in tokenized:\n",
    "    wordsList=word_tokenize(i)\n",
    "    wordsList=[w for w in wordsList if not w in stop_words]\n",
    "    tagged=pos_tag(wordsList)\n",
    "    print(tagged)\n",
    "#DT\" is a tag used in Part-of-speech (POS) tagging to indicate the word is a determiner. Determiners are words that are used to introduce or specify nouns, such as \"the\", \"a\", \"an\", \"this\", \"that\", \"these\", and \"those\"\n",
    "#NN:noun.\n",
    "#JJ:adjective\n",
    "#VBG: verb form that ends with \"-ing\" and can function as a noun or an adjective\n",
    "#RB RB stands for adverb, which is a word that modifies or describes a verb, an adjective, or another adverb. In the sentence \"such as noun\", \"such\" is an adjective and \"as\" is a preposition, but \"noun\" is mistakenly tagged as an adverb. This is likely because the NLTK POS tagger is not always perfect and can make errors or misclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e723cce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Amazon', 'GPE'), ('American', 'GPE'), ('Jeff  Bezos', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "#Named Entity Recognition (NER)\n",
    "from nltk import ne_chunk\n",
    "doc='''Amazon is an American multinational technology company that focuses on e-commerce, cloud computing, digital streaming,\n",
    "and artificial intelligence.Amazon was founded by Jeff Bezos in 1994 as an online bookstore and\n",
    "has since grown to become one of the most valuable companies in the world, with a market capitalization of over $1 trillion.\n",
    " '''\n",
    "#tokenize doc\n",
    "tokenized_doc=word_tokenize(doc)\n",
    "tagged_sentences=pos_tag(tokenized_doc)\n",
    "ne_chunked_sents=ne_chunk(tagged_sentences)\n",
    "#extract all named entities\n",
    "named_entities=[]\n",
    "for tagged_tree in ne_chunked_sents:\n",
    "    if hasattr(tagged_tree,'label'):\n",
    "        entity_name=' '.join(c[0] for c in tagged_tree.leaves())\n",
    "        entity_type=tagged_tree.label()\n",
    "        named_entities.append((entity_name,entity_type))\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70654c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
